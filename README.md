# Vision-2020
Our 2020 vision code (in Python)

## Components
We are using a Raspberry Pi coprocessor and a networking switch to run our vision code. We are also using a USB Camera.

## How to use 
Our strategy is quite similar to the one described [here](https://docs.wpilib.org/en/latest/docs/software/vision-processing/raspberry-pi/using-a-coprocessor-for-vision-processing.html)
However, instead of connecting the switch directly to the roboRIO, we connect it to the radio instead. Then, we use the radio's second ethernet port to connect to the roboRIO.

Follow the documentation here to install the necessary components onto the rPi [here](https://docs.wpilib.org/en/latest/docs/software/vision-processing/raspberry-pi/index.html).

In the FRC Vision webdashboard, adjust the camera's default settings to the json file provided in this repository. This makes vision processing a lot easier because everything that is not bright is already filtered out by simply adjusting camera settings. Make sure you save the settings onto your camera.

Finally, simply deploy this code as a python file in the Application tab of the pi's webdashboard.

## How we coded this
We coded this simply by mixing the python example from the FRC Vision software on the raspberry pi and the pipeline code was generated by GRIP. There is a tutorial on how to use GRIP [here](https://docs.wpilib.org/en/latest/docs/software/vision-processing/grip/index.html). 

## Features
* USB camera connected to raspberry pi streams to Shuffleboard/Smartdashboard
* Vision pipeline processes images from the USB Camera. It pushes values from the vision processing to Networktables.
* Another video that include boxes the target contour is also streamed to SmartDashoard, which allows for easier debugging
